# Transformers

## Description:

This repo is a collection of PyTorch implementations of Transformer architectures with simple flexible config for ease of experimentation. The goal is learning and experimentation.


## Resources:

1. [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
2. [On Layer Normalization in the Transformer Architecture](https://arxiv.org/abs/2002.04745)
3. [minGPT](https://github.com/karpathy/minGPT)
4. [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
5. [d2l-vision-transformer](https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html)
6. [vector-quantize-pytorch](https://github.com/lucidrains/vector-quantize-pytorch)