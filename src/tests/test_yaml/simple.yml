type: 'GPT'
state_path: 'model_state.pt'
params:
  hidden_dim: 12
  num_heads: 4
  dropout: 0.5
  embedder:
    type: 'TokenPositionEmbedder'
    params:
      max_sequence_length: 512
      dictionary_size: 1000
      hidden_dim: 12
  layers:
    - num: 2
      type: 'TransformerLayer'
      params:
        hidden_dim: 12
        attn:
          type: 'Attention'
          params:
            hidden_dim: 12
            num_heads: 4
            dropout: 0.5
        mlp:
          type: 'MLP'
          params:
            hidden_dim: 12
            dropout: 0.5